#!/usr/bin/env python
# vim: set syntax=python

import math
import glob
import subprocess
import argparse
import os.path
import re
import yaml
import os
import pwd
import time
import uuid
import pysam       # just for reading BAM headers to find samples
import snakemake
import distutils.util # for strtobool
import csv
from sys import exit

try:
    from version import githash, buildnum, version
except ImportError:
    version = "__no_version__"
    buildnum = "__no_version__"
    githash = "__no_version__"


# The default set must include ALL POSSIBLE parameters, even ones like BAM
# lists that will be empty at first.  This is because the default dict keys
# are used to determine which command line arguments are not to be stored
# in the configuration database.
scan2_param_defaults = {
    'analysis': 'call_mutations',
    'genome': 'hs37d5',
    'sex': None,
    'chrY': None,
    'is_started': False,
    'gatk': 'gatk3_joint',
    'abmodel_method': 'grid_search',    # or gradient_descent
    'depth_method': 'samtoolsdepth',
    'parsimony_phasing': False,
    'target_fdr': 0.01,
    'rescue_target_fdr': 0.01,
    'phasing_qual_cutoff_quantile': 0.05,
    'snv_min_sc_alt': 2,
    'snv_min_sc_dp': 6,
    'snv_min_bulk_dp': 11, 
    'snv_max_bulk_alt': 0, 
    'snv_max_bulk_af': 1.0,            # 1 means diabled
    'snv_max_bulk_binom_prob': 1.0,    # 1 means disabled
    'snv_exclude_dbsnp': True,
    'snv_cigar_id_score_quantile_cutoff': 0.05,
    'snv_cigar_hs_score_quantile_cutoff': 0.05,
    'indel_min_sc_alt': 2,
    'indel_min_sc_dp': 10,
    'indel_min_bulk_dp': 11, 
    'indel_max_bulk_alt': 0, 
    'indel_max_bulk_af': 1.0,          # 1 means diabled
    'indel_max_bulk_binom_prob': 1.0,  # 1 means disabled
    'indel_cigar_id_score_quantile_cutoff': 0.05,
    'indel_cigar_hs_score_quantile_cutoff': 0.05,
    'indel_exclude_dbsnp': True,
    # This is the GATK4 default. Option not yet exposed to users. Likely
    # that a value >=25 would be good for NovaSeq6000/X WGA data since
    # raw BQs are 2, 12, 23, 37, but then usually BQSR drops this so
    # that the top quality is 31-32.
    # Needs testing.
    'min_base_quality_score': 10,      
    'analyze_snvs': True,
    'analyze_indels': True,
    'abmodel_use_fit': {},
    'abmodel_samples_per_step': 20000,
    'abmodel_hsnp_tile_size': 100,
    'abmodel_hsnp_n_tiles': 250,
    'abmodel_refine_steps': 4,
    'abmodel_gradient_descent_iterations': 10,
    'compute_sensitivity': False,
    'resample_M': 20,
    'mimic_legacy': False,
    'bulk_sample': None,
    'analysis_regions': [],
    'chrs': [],
    'phaser': 'shapeit',
    'shapeit_refpanel': None,  # a directory, so readable_file fails
    'cross_sample_panel': None, # must be readable IF specified, but pipeline should run without it too
    'scripts': '/opt/anaconda1anaconda2anaconda3/lib/scan2',  # also a directory
    'resources': '/opt/anaconda1anaconda2anaconda3/lib/scan2/resources',  # also a directory
    'bam_map': {},             # maps sample name -> bam path
    'amplification_map': {},   # map sample name -> bulk|MDA|PTA|other things in the future..
    'permtool_config_map': {},
    'permtool_matrix_map': {},
    'permtool_n_permutations': 10000,
    'permtool_snv_generation_param': 100000,
    'permtool_indel_generation_param': 1/50,
}

# Unlike params, all files listed here must be readable upon a call
# to validate() or run().
scan2_file_defaults = {
    'snakefile': '/opt/anaconda1anaconda2anaconda3/lib/scan2/Snakefile',
    'ref': None,
    'dbsnp': None,
    'bams': {},         # miscellaneous BAMs; not genotyped as single cells
    'sc_bams': {},      # BAMs for single cells, will be genotyped
    'bulk_bam': None,   # single bulk BAM for phasing, filtering germlines
    'scan2_objects': {},
    'add_muts': None,
    'makepanel_metadata': None,
    'permtool_muts': None,
    'permtool_bedtools_genome_file': None,
    'gatk_vcf': None,
    'eagle_refpanel': {},
    'eagle_genmap': None,
    'user_supplied_phasing': None
}

scan2_defaults = { **scan2_param_defaults, **scan2_file_defaults }


class Analysis:
    """
    A SCAN2 analysis: metadata and actions.  Instantiation requires
    loading configuration data (and potentially Snakemake runtime
    data) from disk.
    """

    def __init__(self, analysis_dir):
        self.analysis_dir = analysis_dir
        self.cluster_log_dir = Analysis.make_logs_path(self.analysis_dir)
        self.config_path = Analysis.make_config_path(self.analysis_dir)
        self.load()

    @staticmethod
    def make_config_path(path):
        return os.path.abspath(os.path.join(path, 'scan.yaml'))

    @staticmethod
    def make_logs_path(path):
        return os.path.abspath(os.path.join(path, 'cluster-logs'))


    @staticmethod
    def create(args):
        """
        Create a new analysis object on disk and return an Analysis
        object pointing to it. Caution: will overwrite a previously
        existing SCAN2 analysis.
        Analysis is initialized with default parameters.
        """
        cpath = Analysis.make_config_path(args.analysis_dir)

        os.makedirs(args.analysis_dir, exist_ok=True)
        os.makedirs(Analysis.make_logs_path(args.analysis_dir),
            exist_ok=True)
        analysis_uuid = uuid.uuid4()   # A random ID
        ct = time.time()
        cd = time.strftime('%Y-%m-%d %H:%M %Z', time.localtime(ct))
        cfg = { 'creator': pwd.getpwuid(os.getuid()).pw_name,
                'create_time': ct,
                'create_date': cd,
                'analysis_uuid': str(analysis_uuid),
                **scan2_defaults
        }

        with open(cpath, 'w') as yfile:
            yaml.dump(cfg, yfile, default_flow_style=False)

        # Return the newly created object
        return Analysis(args.analysis_dir)


    def is_started(self):
        return self.cfg['is_started']


    def __str__(self):
        return "%s SCAN2 analysis type=%s (change with `config --analysis`), ID=%s" % \
            ('LOCKED' if self.is_started() else 'UNLOCKED',
             str(self.analysis),
             str(self.analysis_uuid))


    def show(self, verbose=False):
        """If 'verbose' is given, show all configuration values."""
        if verbose:
            return '\n'.join([ str(self) ] + \
                [ "%25s: %s" % (str(k), str(v)) \
                for k, v in self.cfg.items() ])
        else:
            return '\n'.join([ str(self),
                '%15s: %d single cell(s), %d bulk(s), %d other' % \
                    ('BAMs', len(self.cfg['sc_bams']),
                      self.cfg['bulk_bam'] is not None,
                      len(self.cfg['bams'])),
                '%15s: %s' % ('Creator', self.cfg['creator']),
                '%15s: %s' % ('Create date', self.cfg['create_date']) ])


    def load(self):
        """Requires self.config_path to already be set."""
        with open(self.config_path, 'r') as yf:
            self.cfg = yaml.load(yf, Loader=yaml.FullLoader)
        self.analysis = self.cfg['analysis']
        self.analysis_uuid = self.cfg['analysis_uuid']


    def configure(self, args, verbose=False):
        """
        Merge new parameter values from 'args' into the previous config
        dictionary. New parameter specifications override old ones.  Since
        these values are not changeable once an analysis begins running,
        there is no issue with overwriting old values.

        'args' is the result of ArgumentParser.  Argument values will be
        None unless they were specified on the command line in *this*
        invocation of the scan2 script.
        """
        # First pass: get all new arguments and perform any special
        # handling. Second pass will tally updates and inform.
        new_cfg = {}
        for k, v in vars(args).items():
            if v is not None:
                for k2, v2 in self.handle_special(k, v).items():
                    new_cfg[k2] = v2
        
        # unusually special updates
        # 1. make the default value for chrs/gatk
        # regions when the user specifies the reference FASTA. But only
        # do this is the user did not also specify their own gatk regions
        # via --regions or --region-file in this call AND analysis_regions was
        # not set in a previous configure().
        if 'ref' in new_cfg and \
            'analysis_regions' not in new_cfg and self.cfg['analysis_regions'] == []:
            if args.genome == 'mm10':
                error("Must supply --regions or --regions-file for genome=mm10")
            autosomes = get_autosomes_from_ref(new_cfg['ref'])
            new_cfg = { **new_cfg, **Analysis.handle_regions(autosomes) } # dict merge

        if 'ref' in new_cfg or 'ref' in self.cfg:
            ref = new_cfg['ref'] if 'ref' in new_cfg else self.cfg['ref']
            new_cfg['chrY'] = get_chrY_from_ref(ref)

        updates = 0
        for k, v in new_cfg.items():
            if k in self.cfg.keys() and v is not None and self.cfg[k] != v:
                if args.verbose:
                    print("Updating %s: %s -> %s" % (k, str(self.cfg[k]), str(v)))
                if k in scan2_file_defaults and type(v) == str:
                    # only top-level files are automatically converted,
                    # deeper files require handle_special().
                    v = readable_file(v)   # returns the absolute path
                self.cfg[k] = v
                updates = updates + 1

        # 2. Special update: sample -> bam map
        if updates > 0:
            tups = []
            if self.cfg['bulk_bam'] is not None:
                tups.append((self.cfg['bulk_sample'], self.cfg['bulk_bam']))
            if len(self.cfg['sc_bams']) > 0:
                tups = tups + [ (s, b) for s, b in self.cfg['sc_bams'].items() ]
            if len(self.cfg['bams']) > 0:
                tups = tups + [ (s, b) for s, b in self.cfg['bams'].items() ]
            if len(tups) > 0:
                self.cfg['bam_map'] = dict(tups)
                print(self.cfg['bam_map'])

        if updates == 0:
            print("No changes to make to configuration. Stopping.")
        else:
            with open(self.config_path, 'w') as yf:
                yaml.dump(self.cfg, yf, default_flow_style=False)


    def handle_special(self, key, value):
        """
        Allow special handling of configuration parameters.  Any command
        line argument may expand into an arbitrarily large dict to be added
        to the configuration database.
        To skip special handling, simply return the original key and value.
        """
        if key in [ 'bam', 'sc_bam', 'bulk_bam', 'scan2_object' ]:
            if key in [ 'bam', 'sc_bam' ]:  # these provide lists
                return { key + 's' :
                    dict([ (sample_from_bam(bam), readable_file(bam)) for bam in value ]) }
            if key == 'bulk_bam':
                return { 'bulk_bam': readable_file(value),
                         'bulk_sample': sample_from_bam(value) }
            if key == 'scan2_object':       # list with nargs=2
                return { key + 's' :
                    dict([ (sample_id, readable_file(rda)) for sample_id, rda in value ]) }

        if key == 'permtool_sample':
            return {
                'permtool_config_map' :
                    dict([ (sample_id, readable_file(self.make_config_path(scan2_dir))) for sample_id, scan2_dir in value ]),
                'permtool_matrix_map' :
                    dict([ (sample_id, readable_file(os.path.join(scan2_dir, 'depth_profile', 'joint_depth_matrix.tab.gz'))) for sample_id, scan2_dir in value ]) }

        if key in [ 'regions', 'regions_file' ]:
            regions = regions_from_string(value) if \
                key == 'regions' else regions_from_file(value)
            return Analysis.handle_regions(regions)

        if key == 'abmodel_use_fit':
            # this is a list of tuples
            return { key : dict([ (sample, readable_file(file)) for sample, file in value ]) }

        if key == 'phaser':
            return { 'phaser': value.lower() }   # Just ensure lowercase

        if key in [ 'eagle_refpanel' ]:
            # value = the path to the Eagle ref panel
            # Other sanity checks related to the reference panel are handled
            # in check_phaser().
            return { 'eagle_refpanel' : eagle_refpanel_files(value) }

        if key == 'gatk_vcf':
            # check that there is an associated VCF index file with the expected name
            readable_file(value + '.idx')
            return { 'gatk_vcf': readable_file(value) }

        # just ensure lower case
        if key == 'sex':
            return { 'sex': value.lower() }

        # a list of [sample_ID, amplification method] lists is provided.
        # normalize the amplification method to lower case.
        if key == 'amplification':
            return { 'amplification_map' : dict((k, v.lower()) for k, v in value) }

        # default: use the key=value pair as specified on command line
        return { key: value }


    @staticmethod
    def handle_regions(regions):
        """Extra handling for GATK genotyping intervals (analysis_regions)."""
        return { 'analysis_regions': regions,
                 'chrs': chrs_from_regions(regions) }


    def check_scan2_objects(self):
        if len(self.cfg['scan2_objects']) == 0:
            error('no SCAN2 objects were specified (--scan2-object)')
        [ readable_file(r) for r in self.cfg['scan2_objects'].values() ]


    def check_bams(self, error_on_missing=True):
        if error_on_missing and self.cfg['bulk_bam'] is None:
            error('no bulk BAM was specified (--bulk-bam)')

        if error_on_missing and len(self.cfg['sc_bams']) == 0:
            error('no single cell BAMs were specified (--sc-bam)')

        all_bams = []
        if self.cfg['bulk_bam'] is not None:
            print('adding bulk')
            all_bams += [ self.cfg['bulk_bam'] ]   # not a list like others
        if len(self.cfg['sc_bams']) > 0:
            print('adding SC')
            all_bams += list(self.cfg['sc_bams'].values())
        if self.cfg['bams'] is not None:
            print('adding bam')
            all_bams += list(self.cfg['bams'].values())

        [ readable_file(b) for b in all_bams ]
        # sometimes indexes convert .bam -> .bai rather than appending .bam -> .bam.bai
        #[ readable_file(b + '.bai') for b in all_bams ]


    def check_bams_makepanel(self):
        """
        Differs from above in that makepanel comes with a metadata file
        identifying single cell and bulk bams. Make sure everything in
        the metadata file is present in the list of BAMs.
        """
        donors = []
        samples = []
        amps = []
        with open(self.cfg['makepanel_metadata'], 'r') as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                donors.append(row['donor'])
                samples.append(row['sample'])
                amps.append(row['amp'].lower())

        if len(set(donors)) == 1:
            warning("only a single donor was provided in --makepanel-metadata; indel results will be INVALID.")
            
        if amps.count('bulk') == 0:
            error("no BAMs were annotated as 'bulk' in --makepanel-metadata. At least one bulk BAM is required for panel building.")

        if amps.count('bulk') == len(amps):
            error("All BAMs were annotated as 'bulk' in --makepanel-metadata. At least one single cell BAM is required for panel building.")
            
        # Make sure all sample IDs in the makepanel file exist in BAM map
        bam_samples = []
        if self.cfg['bulk_bam'] is not None:
            bam_samples += [ self.cfg['bulk_bam'] ]   # not a list like others
        if self.cfg['sc_bams'] is not None:
            bam_samples += list(self.cfg['sc_bams'].keys())
        if self.cfg['bams'] is not None:
            bam_samples += list(self.cfg['bams'].keys())
        if not all([ s in bam_samples for s in samples ]):
            error('some samples listed in --makepanel-metadata have no associated BAM file (specified by --bam, --sc-bam, or --bulk-bam). Please ensure that every BAM in the metadata table is present. Note that samples IDs are automatically determined from BAM files by the SM: tag in the first read group (@RG record).')
        

    def check_phaser(self):
        # Determine the phaser and check relevant files.
        if self.cfg['phaser'] == 'shapeit':
            check_shapeit(self.cfg['shapeit_refpanel'])
        elif self.cfg['phaser'] == 'eagle':
            check_eagle(self.cfg['eagle_refpanel'], self.cfg['eagle_genmap'])
        elif self.cfg['phaser'] == 'user_supplied_phasing':
            check_user_supplied_phasing(self.cfg['user_supplied_phasing'])
        else:
            error("--phaser must be either 'shapeit', 'eagle' or 'user_supplied_phasing'\n")


    def check_sex(self):
        if self.cfg['sex'] != 'male' and self.cfg['sex'] != 'female':
            error("--sex must be either 'male' or 'female', but got " + str(self.cfg['sex']) + "\n")


    def validate(self):
        """
        Check that all necessary files exist and ensure the list of
        BAMs and single cell/bulk sample IDs make sense.
        """
        print("Checking --amplification settings..")
        check_amplification(all_samples=self.cfg['bam_map'].keys(),
            amp_map=self.cfg['amplification_map'])

        if self.cfg['analysis'] == 'call_mutations' or self.cfg['analysis'] == 'makepanel':
            # Additional checks: index files
            print("Checking reference genome..")
            check_refgenome(self.cfg['ref'])
            print("Checking dbSNP VCF..")
            check_dbsnp(self.cfg['dbsnp'])
    

            if self.cfg['analysis'] == 'call_mutations':
                # Additional checks: indexes, at least one bulk and one single cell
                print("Ensuring sex assigned at birth is specified..")
                self.check_sex()

                print("Checking BAMs..")
                self.check_bams()

                # Additional checks: very specific reference panel layout
                print("Checking phasing reference panel..")
                self.check_phaser()

            if self.cfg['analysis'] == 'makepanel':
                print("Checking BAMs..")
                self.check_bams(error_on_missing=False)
                print("Checking panel metadata..")
                readable_file(self.cfg['makepanel_metadata'])
                self.check_bams_makepanel()

        if self.cfg['analysis'] == 'rescue':
            print("Checking SCAN2 objects..")
            self.check_scan2_objects()

            if self.cfg['add_muts'] is not None:
                readable_file(self.cfg['add_muts'])

            fdr = float(self.cfg['rescue_target_fdr'])
            if fdr < 0 or fdr > 1:
                error('--rescue-target-fdr must be in [0, 1]')

        if self.cfg['analysis'] == 'permtool':
            print("Checking mutation file..")
            readable_file(self.cfg['permtool_muts'])

            print("Checking bedtools -g file..")
            readable_file(self.cfg['permtool_bedtools_genome_file'])

            print("Checking per-sample SCAN2 config files..")
            for sample, config in self.cfg['permtool_config_map'].items():
                readable_file(config)

            print("Checking depth matrix files..")
            for sample, matrix in self.cfg['permtool_matrix_map'].items():
                readable_file(matrix)



def error(str):
    print("ERROR: " + str)
    exit(1)


def warning(str):
    print("WARNING: " + str)
    exit(1)


# Only checks that a file path can be read.
def readable_file(path):
    try:
        if path is None:
            raise IOError('file is not specified')
        with open(path, 'r') as f:
            return os.path.abspath(path)
    except IOError as err:
        error("file {0} could not be read:\n    {1}".format(path, err))


def check_amplification(all_samples, amp_map):
    """
    Ensure samples in the amplification map are valid sample IDs and
    the amplification types specified are recognized.
    """
    for sample, amp in amp_map.items():
        if sample not in all_samples:
            error("sample '{0}' is not present in any of the supplied BAMs".format(sample))
        if amp not in [ 'pta', 'mda', 'bulk', 'unknown' ]:
            error("amplification type '{0}' is not a recognized method. See scan2 config --help.".format(amp))
    

def check_refgenome(refgenome):
    """Check for FASTA index (.fai) and dictionary (.dict) files.
       Assumes refgenome has already been checked."""
    if refgenome is None:
        error("please provide a reference genome (--ref)")
    readable_file(refgenome + '.fai')
    readable_file(re.sub('.fasta$', '.dict', refgenome))


def get_autosomes_from_ref(refpath):
    """
    Get the correct names for chromosomes 1-22 from the FASTA header.
    ASSUMES the first 22 contigs in the header are the autosomes. mm10 is
    not sorted this way.
    """
    with open(refpath + '.fai', 'r') as f:
        return [ line.split('\t')[0] for line in f ][0:22]


def get_chrY_from_ref(refpath):
    """
    Get the contig name for chrY from the FASTA header.
    For the 2 organisms supported (human and mouse), chrY is haploid
    and thus there is no reference panel for phasing.  SCAN2 needs
    to be aware of this to avoid phasing chrY.
    """
    with open(refpath + '.fai', 'r') as f:
        for line in f:
            chrom = line.strip().split('\t')[0]
            if chrom == "Y" or chrom == "chrY":
                return chrom
    error("Could not find a recognized Y chromosome in reference FASTA index file")


def check_dbsnp(dbsnp):
    """Check for the VCF index file (.idx).  Assumes dbsnp has already
       been checked."""
    if dbsnp is None:
        error("please provide a dbSNP VCF (--dbsnp)")
    readable_file(dbsnp)
    readable_file(dbsnp + '.idx')


def check_user_supplied_phasing(user_supplied_phasing):
    """Just make sure the supplied VCF is gzipped and tabix indexed."""
    if user_supplied_phasing is None:
        error("please provide an indexed variant file readable by bcftools (e.g., .BCF with .csi index; .vcf.gz with .tbi index) to --user-supplied-phasing")
    if not (user_supplied_phasing.lower().endswith(".vcf.gz") or user_supplied_phasing.lower().endswith(".bcf")):
        error("--user-supplied-phasing must be end in '.vcf.gz' or '.bcf'")

    if user_supplied_phasing.lower().endswith(".vcf.gz"):
        readable_file(user_supplied_phasing + ".tbi")
    if user_supplied_phasing.lower().endswith(".bcf"):
        readable_file(user_supplied_phasing + ".csi")


def check_shapeit(shapeit_panel):
    """Check all required SHAPEIT haplotype panel files. Exit if any are missing."""
    if shapeit_panel is None:
        error("no SHAPEIT reference panel provided (--shapeit-refpanel)")
    if not os.path.exists(shapeit_panel):
        error("SHAPEIT panel path does not exist: " + shapeit_panel)
    if not os.path.isdir(shapeit_panel):
        error("SHAPEIT panel path is not a directory; " + shapeit_panel)
    for i in range(1, 23):
        fname = 'genetic_map_chr{0}_combined_b37.txt'.format(i)
        readable_file(os.path.join(shapeit_panel, fname))
        for suf in [ 'hap', 'legend' ]:
            fname = "1000GP_Phase3_chr{0}.{1}.gz".format(i, suf)
            readable_file(os.path.join(shapeit_panel, fname))

    # chrX is not consistently named
    readable_file(os.path.join(shapeit_panel, "1000GP_Phase3_chrX_NONPAR.hap.gz"))
    readable_file(os.path.join(shapeit_panel, "1000GP_Phase3_chrX_NONPAR.legend.gz"))
    readable_file(os.path.join(shapeit_panel, "genetic_map_chrX_nonPAR_combined_b37.txt"))


def eagle_refpanel_files(eagle_refpanel):
    chrs = [ 'chr' + str(chrom) for chrom in list(range(1,23)) + [ 'X' ] ]
    files = {}
    for s in chrs:
        files[s] = os.path.join(eagle_refpanel, '{0}_GRCh38.genotypes.bcf'.format(s))
    return files


def check_eagle(eagle_refpanel, eagle_genmap):
    """This function is called in scan2 validate, which is AFTER the config
    database has been parsed. So eagle_refpanel is a dict() chr -> file path,
    not the path to the top level of the Eagle refpanel dir."""
    if eagle_refpanel is None:
        error("no Eagle reference panel provided (--eagle-refpanel)")

    if eagle_genmap is None:
        error("no Eagle genmap provided (--eagle-genmap)")
    if not os.path.exists(eagle_genmap):
        error("Provided Eagle genmap file does not exist: " + eagle_genmap)

    for f in eagle_refpanel.values():
        readable_file(f)
        readable_file(f + '.csi')


def sample_from_bam(bampath):
    with pysam.AlignmentFile(bampath, 'r') as af:
        sample = af.header.to_dict()['RG'][0]['SM']
    print('Got sample name "%s" from BAM %s' % (sample, bampath))
    return sample




def regions_from_string(regions):
    return args.regions.split(',')


def regions_from_file(regions_file):
    regions = []
    with open(regions_file, 'r') as f:
        for line in f:
            if line.startswith("#"):
                continue

            chrom, start, stop = line.strip().split('\t')[0:3]
            regions.append("{0}:{1}-{2}".format( chrom, int(start)+1, stop))
    return regions


def chrs_from_regions(regions):
    chrs = []
    # retains order
    rchrs = [ r.split(":")[0] for r in regions ]
    for c in rchrs:
        if chrs.count(c) == 0:
            chrs.append(c)

    return chrs






def do_init(args):
    try:
        # If an analysis exists, exception won't be thrown.
        a = Analysis(args.analysis_dir)
        print("ERROR: '%s' already contains a SCAN2 analysis." % \
            args.analysis_dir)
        print("Please delete the directory to create a new analysis.")
        print(a.show())
        exit(1)
    except FileNotFoundError:
        # Nothing on disk. Proceed.
        print("Creating new analysis with default parameters in '%s'.." % \
            args.analysis_dir)
        a = Analysis.create(args)
        print(a.show())
        print('Done.')
        print('Provide input files and set parameters via scan2 config.')
        print('Start the analysis with scan2 run.')


    
def do_config(args):
    a = Analysis(args.analysis_dir)

    if a.is_started():
        error("This analysis has already started and can no longer "
              "be configured.  If you wish to change runtime parameters, "
              "please see the 'run' subcommand.")

    a.configure(args)


def do_validate(args):
    a = Analysis(args.analysis_dir)
    a.validate()


def do_run(args):
    a = Analysis(args.analysis_dir)

    # XXX:
    # As in run_snakemake() below, when snakemake encounters multiple --set-threads
    # arguments, it ignores all but the last.  So if the user passes --set-threads
    # through the --snakemake-args parameter, all values here will be lost.
    # Need a separate --set-threads argument in SCAN2 that collects all values.
    set_threads = [
        '--set-threads', 'abmodel_scatter=' + str(args.abmodel_n_cores),
        'abmodel_gradient_scatter=' + str(args.abmodel_gradient_n_cores),
        'genotype=' + str(args.genotype_n_cores),
        'integrate_tables=' + str(args.integrate_table_n_cores),
        'depth_profile_region_summarize_per_sample=' + str(args.digest_depth_n_cores),
        'scatter_compute_ab_ests_and_models=' + str(args.abests_and_mutmodels_n_cores),
        'sensitivity_abmodel_covariates_scatter=' + str(args.ab_covariates_n_cores)
    ]

    # I would like to use the snakemake API directly (i.e., call snakemake()),
    # but this makes it difficult to allow the user to supply arbitrary
    # snakemake options.
    # Must be a vector of strings as would be expected in argv[1:].
    # N.B., argv[0] is the invoking executable and should not be specified.
    snakemake_command = [
        "--snakefile",  args.snakefile,
        "--configfile", a.config_path,
        "--directory", a.analysis_dir,
        "--latency-wait", "30",
        "--rerun-incomplete",
        "--jobs", str(args.joblimit),
    ] + set_threads

    run_snakemake(snakemake_command, args, a)


# Runs a snakemake command provided by the user. Assumes 'args' contains
# runtime args.
def run_snakemake(snakemake_command, args, a):
    # Add version info to the config object in Snakemake
    #
    # XXX: This might be bad. seems like snakemake does not allow multiple --config
    # options to be passed on the command line, meaning we are preventing users from
    # accessing this through --snakemake-args. Might need to add a special --config
    # option to allow this (i.e., instruct users NOT to use --config via --snakemake-args).
    snakemake_command += [
        "--config", "scan2_version=" + str(version), "scan2_buildnum=" + str(buildnum), "scan2_githash=" + str(githash)
    ]

    if args.memlimit:
        snakemake_command += [ "--resources",  "mem=" + str(args.memlimit) ]

    # handle --cluster or --drmaa and perform %logdir substitution
    if args.cluster is not None or args.drmaa is not None:
        if args.cluster is not None and args.drmaa is not None:
            error('only one of --cluster and --drmaa can be specified')
        if args.cluster is not None:
            snakemake_command += [ '--cluster',
                re.sub('%logdir', a.cluster_log_dir, args.cluster) ]
        else:
            snakemake_command += [ '--drmaa',
                re.sub('%logdir', a.cluster_log_dir, args.drmaa) ]

    # additional arbitrary snakemake args
    if args.snakemake_args:
        if args.snakemake_args[0] == ' ':
            args.snakemake_args = args.snakemake_args[1:]
        snakemake_command += args.snakemake_args.split(' ')

    snakemake.main(snakemake_command)


def do_makepanel(args):
    a = Analysis(args.analysis_dir)

    snakemake_command = [
        "--snakefile",  args.snakefile,
        "--configfile", a.config_path,
        "--directory", a.analysis_dir,
        "--latency-wait", "30",
        "--rerun-incomplete",
        "--jobs", str(args.joblimit),
        "--set-threads", "process_tab=" + str(args.n_cores)
    ]

    run_snakemake(snakemake_command, args, a)


def do_rescue(args):
    a = Analysis(args.analysis_dir)

    snakemake_command = [
        "--snakefile",  args.snakefile,
        "--configfile", a.config_path,
        "--directory", a.analysis_dir,
        "--latency-wait", "30",
        "--rerun-incomplete",
        "--jobs", str(args.joblimit),
        "--set-threads", "mutsig_rescue=" + str(args.n_cores)
    ]

    run_snakemake(snakemake_command, args, a)


def do_permtool(args):
    a = Analysis(args.analysis_dir)

    set_threads = [
        '--set-threads',
        'make_permutations=' + str(args.make_permutations_n_cores),
        'combine_permutations=' + str(args.combine_permutations_n_cores)
    ]

    snakemake_command = [
        "--snakefile",  args.snakefile,
        "--configfile", a.config_path,
        "--directory", a.analysis_dir,
        "--latency-wait", "30",
        "--rerun-incomplete",
        "--jobs", str(args.joblimit)
    ] + set_threads

    run_snakemake(snakemake_command, args, a)


def do_show(args):
    a = Analysis(args.analysis_dir)
    print(a.show(args.verbose))


# Would prefer not to have to parse this output, but learning to use the
# snakemake API directly is a project for a later time.
def parse_snakemake_job_report(process):
    jobdict = {}
    indata = False
    for line in process.stdout.decode().split('\n'):
        line = line.strip()
        if line == '':
            continue

        #if line == 'Job counts:' or line == 'count\tjobs':
        # Now prints something like this:
        # Job stats:
        # job                      count    min threads    max threads
        # ---------------------  -------  -------------  -------------
        # abmodel_gather              11              1              1
        # abmodel_scatter            242              1              1
        if line.startswith('---------'):
            indata = True
            continue

        if indata:
            try:
                # below converts multiple consecutive whitespace characters into
                # one space, then splits on space
                rulename, count, minthr, maxthr = \
                    ' '.join(line.split()).split(' ')
                #count, rulename = line.split('\t')
                jobdict[rulename] = int(count)
            except ValueError:
                # Thrown on the last line, which does not have a rule name
                jobdict['__total'] = int(line)

    # when there's no jobs to run, snakemake prints nothing
    if len(jobdict) == 0:
        jobdict['__total'] = 0

    return jobdict


# 20 block progress bar
def progress_bar(x, total):
    blocks = math.floor(x/total*100/5)
    return '[' + '#'*blocks + ' '*(20-blocks) + '] %5d/%5d %0.1f%%' % (x, total, 100*x/total)


def do_progress(args):
    a = Analysis(args.analysis_dir)
    snakemake_command_status = [ "snakemake",
        "--snakefile",  args.snakefile,
        "--configfile", a.config_path,
        "--directory", a.analysis_dir,
        "--rerun-incomplete",
        "--quiet", "--dryrun" ]

    # Command above returns jobs that currently need to run.
    # Adding --forceall shows how many jobs there were to begin wtih.
    pstatus = subprocess.run(snakemake_command_status, capture_output=True)
    ptotal = subprocess.run(snakemake_command_status + [ '--forceall' ],
        capture_output=True)
    dstatus = parse_snakemake_job_report(pstatus)
    dtotal = parse_snakemake_job_report(ptotal)
    print(a.show())
    print('')
    for k, v in dtotal.items():
        # __total is the total count, all is Snakemake's top-level dummy rule
        if k != 'all':
            n_complete = v - dstatus.get(k, 0)
            k = 'Total progress' if k == '__total' else k
            print('%-40s %s' % (k, progress_bar(n_complete, v)))


def do_vcf(args):
    print("scan2 vcf NOT IMPLEMENTED!")


# Variables here are global
def version_string():
    return("SCAN2 version " + version + ", build #" + buildnum + ", git hash: " + githash)


def do_version(args):
    print(version_string())


##########################################################################
# Command line arguments and subcommand definitions.
##########################################################################

def add_runtime_args(parser):
    parser.add_argument('--joblimit', default=1, metavar='INT',
        help='Allow at most INT jobs to execute at any given time.  For '
            'multicore machines or clusters, this can greatly decrease '
            'runtime.')
    parser.add_argument('--memlimit', default=None, metavar='INT', type=int,
            help="Total available memory in MB.  If unspecified, memory is "
            "treated as unlimited.")
    parser.add_argument('--cluster', default=None, type=str, metavar='ARGS',
        help="Pass ARGS to Snakemake's --cluster parameter.  Do not use "
            "--snakemake-args to access --cluster.  Memory requirements "
            "for each job can be accessed via {resources.mem} and any "
            "instance of '%%logdir' in ARGS will be replaced by "
            "--output-dir/cluster-logs.")
    parser.add_argument('--drmaa', default=None, type=str, metavar='ARGS',
        help="Pass ARGS to Snakemake's --drmaa parameter.  Do not use "
            "--snakemake-args to access --drmaa.  Memory requirements for "
            "each job can be accessed via {resources.mem} and any instance "
            "of '%%logdir' in ARGS will be replaced by "
            "--output-dir/cluster-logs.")
    parser.add_argument('--snakemake-args', default='', type=str, metavar='STRING',
        help='Allows supplying arbitrary snakemake arguments in STRING. See snakemake --help for a list of parameters. Note that a leading space may be necessary, e.g., --snakemake-args " --dryrun".')

    return parser

if __name__ == "__main__":
    ap = argparse.ArgumentParser(prog='scan2',
        description='Somatic SNV genotyper for whole genome amplified '
                    'single cell sequencing experiments.',
        epilog=version_string(),
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    ap.add_argument('-d', '--analysis-dir', metavar='PATH', default='.',
        type=str,
        help='PATH containing the SCAN2 analysis. Required for all subcommands.')
    ap.add_argument('--snakefile', metavar='PATH', type=str,
        default='/opt/anaconda1anaconda2anaconda3/lib/scan2/Snakefile',
        help='Path to the main Snakefile.  Unlikely to be necessary for standard use.')
    subparsers = ap.add_subparsers(dest='subcommand')


    ##########################################################################
    # The 'init' subcommand
    #
    # 'init' only creates a new repository, nothing else. It is a separate
    # command to avoid accidental overwrites.
    ##########################################################################
    init_parser = subparsers.add_parser('init',
        help='Initialize a SCAN2 analysis directory')
    init_parser.set_defaults(executor=do_init)
    
    
    ##########################################################################
    # The 'config' subcommand
    #
    # Configuration variables need to be split into ones that can be
    # modified after analysis has run vs. ones that must be set first.
    #
    # Important: overwrite protection.
    ##########################################################################
    config_parser = subparsers.add_parser('config',
        help='Change configuration of a SCAN2 analysis. Note: config parameters '
            'cannot be changed after the analysis has begun.')
    config_parser.set_defaults(executor=do_config)
    
    config_parser.add_argument('--analysis', default='call_mutations',
        choices=[ 'call_mutations', 'makepanel', 'rescue', 'permtool' ],
        help='Analysis to run.\ncall_mutations: call somatic mutations.\nmakepanel: create a cross-sample panel suitable for --cross-sample-filter.\nrescue: perform signature-based rescue on SCAN2 objects generated by call_mutations.\npermtool: create control background distributions for somatic mutations by permutation. Mutations must be preprocesed to, e.g., remove duplicates.')
    config_parser.add_argument('--verbose', action='store_true', default=False,
        help='Print detailed list of all configuration changes.')
    config_parser.add_argument('--scripts', metavar='PATH',
        default='/opt/anaconda1anaconda2anaconda3/lib/scan2',
        help='Path to SCAN2 script files.  Usually points to an installed '
            'set of files.')
    config_parser.add_argument('--resources', metavar='PATH',
        default='/opt/anaconda1anaconda2anaconda3/lib/scan2',
        help='Path to SCAN2 resource.  Usually points to an installed '
            'set of files.')
    config_parser.add_argument('--gatk', default='gatk3_joint',
        choices=[ 'gatk3_joint', 'gatk4_joint', 'gatk4_gvcf', 'sentieon_joint', 'sentieon_gvcf' ],
        help='GATK version and strategy for generating candidate somatic '
             'mutations and cross-sample support. '
             'In SCAN2, "joint" calling refers to the case when BAMs from all samples '
             'are provided directly to HaplotypeCaller.  In the GVCF strategy, '
             'each BAM (sample) is initially analyzed separately and combined '
             'later in a step called "joint calling" by the GATK team. The GVCF '
             'method can be lossy since all BAM data is not stored '
             'in the initial analysis; however, it does not require rerunning GATK '
             'across all samples in the multi-sample mode. The GVCF strategy is '
             'currently experimental and we do not recommend using it. When using'
             'sentieon modes, the user must ensure the `sentieon` binary is in $PATH'
             'and that the environment variable SENTIEON_LICENSE points to a running'
             'sentieon license server.'
             'When --gatk-vcf VCF is used, the user must ensure that the chosen --gatk '
             'strategy matches the strategy used to generate VCF.')

    config_parser.add_argument('--gatk-vcf', type=str, metavar='VCF',
        help='SCAN2 runs GATK twice internally: once with a high read mapping quality '
             'threshold (mmq60) and once with a very low threshold (mmq1). Specifying '
             '--gatk-vcf skips the mmq60 run and uses VCF in its place, potentially '
             'saving a large amount of compute time. This option is especially useful '
             'when `scan2 makepanel` has been run to create a --cross-sample-panel: the '
             'GATK VCF stored in path/to/makepanel_output/gatk/hc_raw.mmq60.vcf is an '
             'appropriate input for --gatk-vcf.\n'
             'VERY IMPORTANT: VCF _must_ be generated by GATK in '
             '_exactly_ the way SCAN2 expects; this option is not meant to allow users '
             'to supply arbitrary inputs to SCAN2. DO THIS AT YOUR OWN RISK! There is '
             'no guarantee that SCAN2 will behave or even call mutations with high '
             'accuracy if the user ignores this advice and supplies a VCF generated by '
             'other means via --gatk-vcf. Furthermore, the --gatk strategy must '
             'match the strategy originally used to generate VCF.')
    config_parser.add_argument('--mimic-legacy', action='store_true', default=False,
        help="Mimic some behaviors of older SCAN2 versions. Not comprehensive.")
    config_parser.add_argument('--compute-sensitivity', default=False, action='store_true',
        help='Model somatic mutation detection sensitivity as a function of genomic position. '
             'If this option is enabled, final SCAN2 objects will be output in sensitivity/<sample name>/scan2_object.rda '
             'rather than the usual call_mutations/<sample name>/scan2_object.rda. SCAN2 objects '
             'with sensitivity models are much larger than standard SCAN2 objects. [THIS PARAMETER IS CURRENTLY IGNORED, SENSITIVITY IS ALWAYS COMPUTED]')

    # required arguments
    req = config_parser.add_argument_group("External data")
    req.add_argument('--sex', type=str, choices=[ 'male', 'female'],
        help='The sex assigned at birth for this individual.  Only affects '
             'results if sex chromosomes (X or Y for human data) are '
             'analyzed.  In particular, when sex=male, the non-PAR regions '
             'of the sex chromosomes are hemizygous and thus allele balance '
             'need not be modelled (there is only one allele, so AB=1 is '
             'always true).  The allele balance model would automatically '
             'reduce to this case (with the exception of statistical '
             'phasing errors and regions of low SNP density) if not for a small technical detail: phasers '
             'can output genotypes in the form "1|1" in hemizygous '
             'regions, which is regarded by the allele balance model as a '
             'non-informative site.  Thus, rather than reducing to AB=1, '
             'SCAN2 will discard nearly every germline variant in '
             'hemizygous regions and thus have no training sites at which '
             'to build the AB=1 model.')
    req.add_argument('--ref', type=readable_file, metavar='PATH',
        help='Path to reference genome FASTA file.  As required by GATK, '
             'two additional files must be present: an index (.fai) and a '
             'dictionary (.dict).')
    req.add_argument('--genome', type=str,
        choices=[ 'hs37d5', 'hg38', 'mm10' ],
        help='Reference genome to use.  This is not checked against '
             'the provided reference genome or phasing panels, the user '
             'must ensure these match the supplied genome.  It is used '
             'when determining mutation signatures (e.g., to fetch '
             'trinucleotide contexts around candidate mutations).  Valid '
             'values must match those recognized by the R SCAN2 package '
             '(see the genome.string.to.bsgenome.object function).')
    req.add_argument('--dbsnp', type=readable_file, metavar='PATH',
        help='Path to a tribble-indexed (NOT tabix indexed) dbSNP VCF.')
    
    infiles = config_parser.add_argument_group('Input sequencing data',
        'At least two BAM files must be specified: one via --bulk-bam'
        ' and at least via one via --sc-bam.  Additional BAMs can be '
        ' specified by --bam: these will be used for GATK joint calling,'
        'but will affect the analysis in no other way.  This can be '
        'useful for adding additional bulks for followup comparisons.'
        'IMPORTANT: BAM files must be indexed (.bai) and must contain'
        ' only a single sample, identified by an @RG line with an SM '
        ' tag in the header.')
    infiles.add_argument('--bulk-bam', metavar='PATH',
        help='Matched bulk sample (not-single cell) for removing '
            'germline or clonal SNVs.  Only one sample may be '
            'designated as the bulk sample.')
    infiles.add_argument('--sc-bam', metavar='PATH', action='append',
        help='BAM corresponding to single cells that are to be '
            'analyzed.  May be specified multiple times.')
    infiles.add_argument('--bam',  action='append', metavar='PATH',
        help='Additional BAM files that will be included in GATK\'s'
            'joint genotyping, but will otherwise not affect the '
            'analysis.  May be specified several times.')
    infiles.add_argument('--amplification', nargs=2, action='append', metavar=('SAMPLE_ID', '{pta|mda|bulk|other}'),
        help="Indicate the type of amplification used to generate sample SAMPLE_ID.  "
            "SAMPLE_ID must match a sample ID specified in the header of one of the "
            "supplied BAM files.")
    
    
    
    gatk = config_parser.add_argument_group('Analysis intervals',
        'These parameters allow the user to specify which portions of the genome should be analyzed and how to parallelize jobs.  The default is to analyze the first 22 contigs in the user-supplied reference genome (and parallelize at the chromosome level); THIS IS NOT RECOMMENDED BECAUSE THE PARALLELIZATION IS NOT GRANULAR ENOUGH.  Regions MUST be specified in the same order as the reference genome and must not overlap!  The user is responsible for ensuring that only diploid, phaseable autosomes and known haploid sex chromosomes are among the list of regions.  We do not recommend analysis of pseudoautosomal regions (PARs), but the user is allowed to specify them.')
    gatk.add_argument('--regions', metavar="STRING",
        help='A comma separated list of regions in the format: chr:start-stop, e.g. 22:30000001-31000000,22:31000001-32000000.  Cannot be specified in addition to --regions-file.  Regions must be 1-indexed, unlike --regions-file, which is a BED file (0-indexed), and both ends of the interval are inclusive.')
    gatk.add_argument('--regions-file', metavar='PATH',
        help='A 0-indexed BED file containing regions to be analyzed.  Cannot be specified in addition to --regions.  Also note that --regions is 1-indexed while intervals in PATH are expected to be 0-indexed.')
    
    
    
    caller = config_parser.add_argument_group("Somatic SNV calling parameters. A minimum requirement of 2 reads supporting a putative mutation is probably good practice at most sequencing depths.  However, the minimum total depth for single cell and bulk may need to be altered.  The defaults of 6 and 11, respectively, were successful on single cells with >25x and bulk with >30x mean coverage.")
    caller.add_argument('--target-fdr', type=float, metavar='FLOAT',
        help='Desired false discovery rate (FDR), a number between 0 and 1 that represents the fraction of the final call set that is expected to be false positives.  This is not formal FDR control via, e.g., q-values.  In general, lower values will increase specificity at the cost of sensitivity.')
    caller.add_argument('--snv-min-sc-alt', type=int, metavar='INT',
        help='Reject somatic SNVs with fewer than INT reads carrying the mutation in single cells.')
    caller.add_argument('--snv-min-sc-dp', type=int, metavar='INT',
        help='Reject somatic SNVs covered by fewer than INT reads in single cells.')
    caller.add_argument('--snv-min-bulk-dp', type=int,  metavar='INT',
        help='Reject somatic SNVs covered by fewer than INT reads in bulk.')
    caller.add_argument('--snv-max-bulk-alt', type=int,  metavar='INT',
        help='Reject somatic SNVs supported by more than INT reads in bulk. IMPORTANT: increasing beyond the default (0) also causes SCAN2 to allow bulk sites with GATK genotype string 0/1 when normally only 0/0 is allowed.')
    caller.add_argument('--snv-max-bulk-af', type=float,  metavar='FLOAT',
        help='Reject mosaic SNVs with VAF > FLOAT in bulk. IMPORTANT: decreasing beyond the default (1) also causes SCAN2 to allow bulk sites with GATK genotype string 0/1 when normally only 0/0 is allowed.  Set to 1 to disable this filter.')
    caller.add_argument('--snv-max-bulk-binom-prob', type=float,  metavar='FLOAT',
        help='Use a one-sided binomial test on bulk data to derive a probability that a candidate site has VAF=50%% in bulk. The alternative hypothesis is that VAF < 50%% (e.g., a potential mosaic). Reject mosaic SNVs with probability >= FLOAT in bulk. Napkin math assuming 2M het germline SNPs per individual and a uniform distribution of probabilities (which holds if the null is true) gives (prob cutoff) = (# of errors committed)/2M. E.g., to commit 1 error (i.e., misclassification of a germline het variant as a somatic mosaic), prob cutoff = 1/2M, or --max-bulk-af=0.0000005. IMPORTANT: decreasing beyond the default (1) also causes SCAN2 to allow bulk sites with GATK genotype string 0/1 when normally only 0/0 is allowed. Set to 1 to disable this filter.')
    caller.add_argument('--indel-min-sc-alt', type=int, metavar='INT',
        help='Reject somatic indels with fewer than INT reads carrying the mutation in single cells.')
    caller.add_argument('--indel-min-sc-dp', type=int, metavar='INT',
        help='Reject somatic indels covered by fewer than INT reads in single cells.')
    caller.add_argument('--indel-min-bulk-dp', type=int,  metavar='INT',
        help='Reject somatic indels covered by fewer than INT reads in bulk.')
    caller.add_argument('--indel-max-bulk-alt', type=int,  metavar='INT',
        help='Reject somatic indels supported by more than INT reads in bulk. IMPORTANT: increasing beyond the default (0) also causes SCAN2 to allow bulk sites with GATK genotype string 0/1 when normally only 0/0 is allowed.')
    caller.add_argument('--indel-max-bulk-af', type=float,  metavar='FLOAT',
        help='Reject mosaic indels with VAF > FLOAT in bulk.  IMPORTANT: decreasing beyond the default (1) also causes SCAN2 to allow bulk sites with GATK genotype string 0/1 when normally only 0/0 is allowed.  Set to 1 to disable this filter.')
    caller.add_argument('--indel-max-bulk-binom-prob', type=float,  metavar='FLOAT',
        help='See note for --snv-max-bulk-binom-prob.  Note that there are typically ~10-fold fewer germline indel variants than SNPs.  Following the napkin math for SNVs: to target 1 misclassification error, set --max-bulk-af=0.000005.')
    caller.add_argument('--min-base-quality-score', type=int,
        help="Passed to GATK HaplotypeCaller's --min-base-quality-score. "
             "Currently only used in GATK4 GVCF.")

    caller.add_argument('--cross-sample-panel', type=readable_file, metavar='PATH',
        help='A table of alt alleles detected over a large number of single cells and bulks. Currently used only for indel filtering but should probably also be used for SNV filtering. Indel calling is NOT precise without a panel of normals.')
    
    
    phasing = config_parser.add_argument_group('Phasing options',
        'SCAN2 can perform phasing internally (using SHAPEIT2 or Eagle) for human samples. Alternatively, the user can supply an index, phased variant file (i.e., vcf.gz or .bcf) with phase determined from external methods. This is the only method available for non-human samples.')
    phasing.add_argument('--phaser', type=str,
        choices=[ 'shapeit', 'eagle', 'user_supplied_phasing' ],
        help='SHAPEIT2 does not provide a reference panel aligned to hg38. If your data are aligned to this genome version, please use Eagle instead. Eagle is currently untested for GRCh37, but may work. To skip population-based phasing, --phaser=user_supplied_phasing allows a phased VCF (using 0|1, 1|0 and 1|1 phased genotypes) to be supplied via --user-supplied-phasing. The VCF must be bgzipped and tabix indexed and contain only germline SNPs and short indels with phase 0|1, 1|0 or 1|1. Homozygous phased sites (1|1) are ignored except when analyzing haploid regions (e.g., male chrX and chrY).  The user_supplied_phasing approach can be advantageous when external information can provide better phasing than statistical phasing based on a reference panel; e.g.: crossbred mouse strains, human samples with mother-father-sibling trio to infer phase or samples for which long-read data can provide read-backed phasing via physical linkage across most of the genome.')
    phasing.add_argument('--phasing-qual-cutoff-quantile', default=0.05, metavar='FLOAT',
        help="Prior to phasing with Eagle or SHAPEIT, remove the bottom FLOAT percentile of sites. Quantiles are computed separately for germline SNVs and indels and is performed per-chromosome. Values must be between 0 and 1.")
    phasing.add_argument('--shapeit-refpanel', metavar='DIR',
        help='Path to the 1000 genomes project phase 3 SHAPEIT panel.  At this time, other phasing panels are not supported. This panel is required if phaser=shapeit.')
    # No longer does anything
    #phasing.add_argument('--parsimony-phasing', action='store_true',
        #help='Choose the phase of each training hSNP i to minimize |VAF_(i-1) - VAF_i|.  This overrides population-based phasing and does not produce well-phased mutations.  Instead, it ensures that switching errors (caused by panel phasing) in regions with allelic imbalance do not inflate model variance or lead to "ping-pong" allele balance estimates.')
    phasing.add_argument('--eagle-refpanel', metavar='DIR',
        help='Path to the an Eagle compatible phasing panel. The reference panel is a set of BCF files, one per chromosome. File names must match the pattern ${chromosome}*.bcf. The user should take care to ensure that chromosome names contain the "chr" prefix to properly match the supplied reference genome. This parameter is required if phaser=eagle.')
    phasing.add_argument('--eagle-genmap', metavar='DIR',
        help='Path to Eagle\'s provided genetic map (can be found under the "tables" directory in the Eagle tarball). This is required if phaser=eagle.')
    phasing.add_argument('--user-supplied-phasing', type=readable_file, metavar='.vcf.gz|.bcf',
        help='A bgzipped, tabix-indexed VCF file or .csi-index BCF file containing phased genotypes covering the majority of the genome for this individual. The VCF should contain only 10 columns, with no INFO fields and only a FORMAT/GT tag for a single sample with values 0|1, 1|0 or 1|1.')
    
    
    abmodel = config_parser.add_argument_group("AB model fitting",
        "These parameters control the method used to fit the parameters of the AB correlation function for each chromosome.  This is by far the most time consuming step of the calling process.  None of these parameters are used for subsequent AB inference.")
    abmodel.add_argument('--abmodel-use-fit', nargs=2, action='append', metavar=('SAMPLE_ID', 'FITS.RDA'),
        help='Do not fit the AB model for sample SAMPLE_ID; instead, use the fits provided in FITS.RDA. This option may be specified multiple times.')
    abmodel.add_argument('--abmodel-method', default='grid_search',
        choices=[ 'grid_search', 'gradient_descent' ],
        help='We offer two methods for estimating the AB model parameters, both of which search for a maximum likelihood estimator: a multi-step grid search and gradient descent via L-BGFS. Grid search is very expensive, but is currently the most reliably accurate method and is thus the default. The total cost of grid search is ~200 CPU hours per single cell. Gradient descent is far more efficient, requiring about ~20 CPU hours per single cell, but sometimes fails to find solutions with likelihood values as large as the grid search.')
    abmodel.add_argument('--abmodel-samples-per-step', type=int, metavar='INT',
        help='Sample the AB model log-likelihood function INT times during each parameter space refinement step.  This parameter applies only to --abmodel-method=grid_search.')
    abmodel.add_argument('--abmodel-refine-steps', type=int, metavar='INT',
        help='Refine the parameter space for random sampling INT times.  After the first log-likelihood sampling has completed, a new parameter space is defined so as to contain the 50 points with highest log likelihood.  This parameter applies only to --abmodel-method=grid_search.')
    abmodel.add_argument('--abmodel-gradient-descent-iterations', type=int, default=10, metavar='INT',
        help='To increase the probability of finding a global optimum (MLE), gradient descent is performed INT times, each with a randomly chosen initial parameter set.  On PTA-amplified single cells, 10 iterations is sufficient to attain a parameter set with equal likelihood to the grid search; however, MDA cells and some PTA cells with erratic amplification qualities can require as many as 50 iterations to attain parameters comparable to grid search.  This option has no effect unless --abmodel-method=gradient_descent.')
    abmodel.add_argument('--abmodel-hsnp-tile-size', type=int, metavar='INT',
        help='Approximate the AB model likelihood function for each chromosome by breaking hSNPs into non-overlapping tiles of size INT.  Larger values significantly increase runtime, but may be necessary for organisms with high SNP density (e.g., >1 hSNP per kilobase on average).  This parameter applies to both --abmodel-method=grid_search and gradient_descent.')
    
    
    spikein = config_parser.add_argument_group("hSNP spike-ins",
        'hSNP spike ins provide a way to estimate sensitivity and the effects of various filters.  hSNP spikeins are also used to train models for excess indel and read clipping, so at least a few thousand spikeins are desired.')
    spikein.add_argument('--resample-M', type=int, metavar="INT",
        help='Parameter for rejection sampling. Larger M will produce fewer hSNP spikein samples for sensitivity and total mutation burden estimation. However, M must be large enough to enable proper rejection sampling.')
    
    
    rescue = config_parser.add_argument_group("Signature-based somatic mutation rescue (scan2 rescue)")
    rescue.add_argument('--scan2-object', nargs=2, action='append', metavar=('SAMPLE_ID', 'RDA'),
        help='SCAN2 object files as generated by call_mutations. SAMPLE_ID must '
             'match the @single.cell property of the object stored in RDA. This '
             'argument can be specified multiple times to create a batch.')
    rescue.add_argument('--add-muts', metavar='CSV', type=readable_file,
        help='CSV file containing somatic mutations, one per line, to add to '
             'the batch for calculating the true mutation specrum. The file must '
             'contain at least two columns: muttype (entries in this column can '
             'have value "snv" or "indel") and mutsig (values in this column must '
             'be SBS96- or ID83-channel values; e.g. ACC:C>A for SBS96 or '
             '3:Del:R:0 for ID83)')
    rescue.add_argument('--rescue-target-fdr', metavar='FLOAT', type=float,
        help='Target FDR for mutation signature rescue. Subject to the same '
             'interpretation and caveats as --target-fdr.')


    makepanel = config_parser.add_argument_group("Cross-sample filter creation (scan2 makepanel)")
    makepanel.add_argument('--makepanel-metadata', metavar='CSV', type=readable_file,
        help='CSV file containing metadata describing the samples in the panel. '
             'Must contain columns named: sample, donor and amp. The amp column '
             'is used to determine bulk vs. single cell status; amp=bulk (case '
             'sensitive) for bulks and any other value for single cells.')


    permtool = config_parser.add_argument_group("Mutation permutation for estimating background mutation rates.")
    permtool.add_argument('--permtool-muts', metavar='CSV', type=readable_file,
        help='CSV file containing all somatic calls to be permuted. Should contain '
             'at lease the following columns: sample, chr, pos, refnt, altnt, pass, '
             'rescue. Required.')
    permtool.add_argument('--permtool-n-permutations', default=10000, type=int, metavar='N',
        help='Number of shuffled mutation sets to create.')
    permtool.add_argument('--permtool-sample', nargs=2, action='append', metavar=('SAMPLE_ID', 'SCAN2_DIR'),
        help='Each sample to be permuted must be supplied explicitly by specifying '
             'this parameter. SCAN2_DIR must point to the top level of a SCAN2 '
             'directory created by the call_mutations pipeline. At least one '
             '--permtool-sample is required for `scan2 permtool`.')
    permtool.add_argument('--permtool-snv-generation-param', default=100000, type=int, metavar='N',
        help='Number of random SNVs to generate per iteration. Enough random sites must be generated so that the mutation spectrum of the random sites contains at least one copy of the input spectrum. Ideally, an iteration should contain 10-100 copies. Increase this parameter if directed by permtool or if your mutations contain a large number of rare types (particularly CpGs).')
    permtool.add_argument('--permtool-indel-generation-param', default=1/50, type=float, metavar='FLOAT',
        help='Similar to --permtool-snv-generation-param but acts as a multiplier for a hard-coded base number of random indels to generate per iteration.')
    permtool.add_argument('--permtool-bedtools-genome-file', type=readable_file, metavar='PATH',
        help='Genome file formatted for the -g option of bedtools intersect.')




    ##########################################################################
    # The 'validate' subcommand
    #
    # Tries to ensure the configuration is complete.
    validate_parser = subparsers.add_parser('validate',
        help='Validate the configuration parameters of a SCAN2 analysis.'
            '  [NOTE: not necessarily exhaustive.]')
    validate_parser.set_defaults(executor=do_validate)
    
    

    ##########################################################################
    # The 'call_mutations' subcommand
    #
    # This runs a call_mutations "first pass" analysis
    run_parser = subparsers.add_parser('call_mutations',
        help='Run a fully configured SCAN2 call_mutations analysis, which is '
            ' sometimes referred to as the "first pass".  Always run the '
            '"validate" subcommand first to ensure that the configuration is '
            'valid.')
    run_parser.set_defaults(executor=do_run)
    run_parser = add_runtime_args(run_parser)
    run_threading = run_parser.add_argument_group("Multithreading options.  Some jobs that can be multi-threaded are not listed here.  Threads for those jobs can be set using the Snakemake syntax --set-threads RULE=N.")
    run_threading.add_argument('--abmodel-n-cores', default=10, type=int, metavar='INT',
        help='Multithread with INT cores when fitting the AB model parameters.  This is the preferred way of fitting the AB model and contrasts with the chunk strategy')
    run_threading.add_argument('--abmodel-gradient-n-cores', default=10, type=int, metavar='INT',
        help='Multithread with INT cores when fitting the AB model parameters using gradient descent.  This is an experimental method for fitting the model.')
    run_threading.add_argument('--genotype-n-cores', default=1, type=int, metavar='INT',
        help="Use INT cores in the final genotyping step.")
    run_threading.add_argument('--integrate-table-n-cores', default=10, type=int, metavar='INT',
        help="Use INT cores when creating the integrated table.")
    run_threading.add_argument('--digest-depth-n-cores', default=10, type=int, metavar='INT',
        help="Use INT cores to tabulate single cell and bulk sequencing depth.")
    run_threading.add_argument('--abests-and-mutmodels-n-cores', default=2, type=int, metavar='INT',
        help="Use INT cores per chromosome to compute AB estimates and mutation models.")
    run_threading.add_argument('--ab-covariates-n-cores', default=2, type=int, metavar='INT',
        help="Use INT cores per chromosome to compute AB estimates and mutation models.")


    ##########################################################################
    # The 'show' subcommand
    #
    # Show all configuration information. Maybe allow for serialization?
    # Could just dump the YAML.
    show_parser = subparsers.add_parser('show',
        help='Show the configuration of a SCAN2 analysis.')
    show_parser.set_defaults(executor=do_show)
    show_parser.add_argument('--verbose', action='store_true', default=False,
        help='Show all configuration variables and settings.')
    
    
    ##########################################################################
    # The 'progress' subcommand
    #
    # Print some information about the progress of the run:
    #   #Jobs complete / Total #Jobs
    #   #Jobs with errors
    progress_parser = subparsers.add_parser('progress',
        help='Prints statistics regarding the progress of a SCAN2 run.')
    progress_parser.set_defaults(executor=do_progress)


    ##########################################################################
    # The 'makepanel' subcommand
    #
    # Runs GATK HaplotypeCaller on a large set of BAMs, usually across cells
    # from multiple individuals. The goal is to generate a matrix of read
    # support for reference and alternate alleles at all sites with evidence
    # of non-reference bases.
    #
    # makepanel runs an analysis, so runtime_args are appropriate.
    makepanel_parser = subparsers.add_parser('makepanel',
        help='Run GATK HaplotypeCaller jointly. Produces files necessary for '
             'indel calling (particularly, the cross-sample filter). IMPORTANT: '
             'If `makepanel` output is not directly used as input to SCAN2 '
             'mutation calling, then the user should ensure that identical '
             '--gatk options are used between makepanel and run.')
    makepanel_parser.set_defaults(executor=do_makepanel)
    makepanel_parser = add_runtime_args(makepanel_parser)
    makepanel_threading = makepanel_parser.add_argument_group("Multithreading options")
    makepanel_threading.add_argument('--n-cores', type=int, default=10, metavar='N',
        help='Number of cores to use when digesting cross-sample panel GATK output. Note that the most consequential parallelization for `makepanel` is the size of the windows passed as --analysis-regions.')
    
    
    ##########################################################################
    # The 'rescue' subcommand
    #
    # Runs SCAN2 somatic mutation rescue procedures. Currently, rescue is
    # performed using mutation signatures.
    #
    # rescue runs an analysis, so runtime_args are appropriate.
    rescue_parser = subparsers.add_parser('rescue',
        help='Run SCAN2 mutation signature-based rescue to call additional '
             'somatic SNVs and indels that did not meet the requirements for '
             'VAF-based calling, but are nevertheless unlikely to be '
             'artifactual.')
    rescue_parser.set_defaults(executor=do_rescue)
    rescue_parser = add_runtime_args(rescue_parser)
    rescue_threading = rescue_parser.add_argument_group("Multithreading options")
    rescue_threading.add_argument('--n-cores', type=int, default=10, metavar='N',
        help='Parallelize somatic mutation rescue over N cores. Each core reads '
             'and processes a full SCAN2 object. For typical human cells, about '   
             '6 GB of RAM is necessary per core if not used on summary.SCAN2 objects.')


    ##########################################################################
    # The 'permtool' subcommand
    #
    # Permutes mutations to create background mutation rates for enrichment
    # testing.
    #
    # permtool runs an analysis, so runtime_args are appropriate.
    permtool_parser = subparsers.add_parser('permtool',
        help='Shuffle somatic mutations around the part of the genome callable '
             'by SCAN2. Also ensures that shuffled mutations match in mutation '
             'spectrum. Importantly, the user must preprocess somatic mutations '
             'to remove, e.g., duplicated sites. SCAN2\'s digest_calls.R script '
             'performs some recommended preprocessing steps. Memory required for '
             'this pipeline is around 4 GB per core. During the '
             'make_permutations phase, bedtools shuffle can require ~2 GB of '
             'memory per thread at peak depending on the size of the callable '
             'regions file.')
    permtool_parser.set_defaults(executor=do_permtool)
    permtool_parser = add_runtime_args(permtool_parser)
    permtool_threading = permtool_parser.add_argument_group("Multithreading options")
    permtool_threading.add_argument('--make-permutations-n-cores', default=10, type=int, metavar='N',
        help="Number of cores to use when generating shuffled mutation candidates.")
    permtool_threading.add_argument('--combine-permutations-n-cores', default=10, type=int, metavar='N',
        help="Number of cores to use when gathering per-sample permutations.")


    ##########################################################################
    # The 'vcf' subcommand
    #
    # Converts a scan2 object into a vcf file.
    vcf_parser = subparsers.add_parser('vcf',
        help='[NOT IMPLEMENTED] Convert a SCAN2 full analysis object to VCF format.')
    vcf_parser.add_argument('scan2_object', type=readable_file, metavar='PATH',
        help='PATH to a SCAN2 object to convert')
    vcf_parser.add_argument('vcf', type=str, metavar='PATH',
        help='Write output VCF to PATH.')
    vcf_parser.add_argument('--simple-filters', action='store_true', default=False,
        help='Choose only one filter (in a fairly arbitrary, though consistent order) to report in the VCF file. Greatly reduces runtime.')
    vcf_parser.set_defaults(executor=do_vcf)


    ##########################################################################
    # The 'version' subcommand
    #
    # Just prints version info.
    version_parser = subparsers.add_parser('version',
        help='Print version info about the SCAN2 pipeline. Note that versions of the SCAN2 pipeline and SCAN2 R package are tracked separately.')
    version_parser.set_defaults(executor=do_version)


    # Get the args and run the relavent subcommand
    args = ap.parse_args()
    if args.subcommand is None:
        print("ERROR: a valid subcommand must be provided. See below.")
        ap.print_help()
        exit(1)
    
    if args.subcommand != 'vcf':
        args.analysis_dir = os.path.abspath(args.analysis_dir)
    args.executor(args)
