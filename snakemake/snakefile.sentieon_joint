# vim: syntax=python

rule gatk_scatter:
    input:
        bam=expand("{bam}", bam=config['bam_map'].values()),
    output:
        vcf=temp("gatk/hc_raw.mmq{gatk_mmq}.region_{analysis_region}.vcf"),
        vcfidx=temp("gatk/hc_raw.mmq{gatk_mmq}.region_{analysis_region}.vcf.idx")
    log:
        "gatk/hc_raw.mmq{gatk_mmq}.region_{analysis_region}.log"
    benchmark:
        "gatk/scatter_benchmark.mmq{gatk_mmq}.region_{analysis_region}.tsv"
    params:
        bamlist=expand("-i {bam}", bam=config['bam_map'].values()),
        regionflag="--interval {analysis_region}",
        mmq="{gatk_mmq}"
    resources:
        # OLD NOTES FROM FIRST ATTEMPTS --------------------------------------------------
        # N.B. scaling 4^x: the vast majority (95%+) of jobs use very, very little memory.
        # However, when a big memory job comes along (usually a handful per panel), its
        # memory usage explodes. E.g., in a test panel of 12 PTA single cells and 4 bulks,
        # 90% of chunks used <625MB, 99% of chunks used <963MB, but the top 5 jobs used
        #                1      2      3      4      5
        #        MB: 64000   3436   2386   1781   1681
        # So, RAM usage must grow very fast to have any chance of handling these in a
        # reasonable amount of time.
        # --------------------------------------------------------------------------------
        #
        # IMPORTANTLY: this implies that we expect to always fail (and restart) at least
        # a few HaplotypeCaller jobs. Since each GATK chunk acts as a checkpoint (i.e.,
        # once a chunk completes, other chunk failures won't ever undo the completed chunk),
        # chunks for panel building should always be very fine (5000-10000 chunks is
        # reasonable)! This is true even if SCAN2 is run on a single core machine where
        # the chunks cannot be parallelized!
        # 
        # After building panels for 16, 127 and 260 ~30x WGS BAMs, found that mem
        # usage scales such that 500 MB + (31 MB * #bams) is sufficient for 99% of chunked
        # GATK jobs. However, the maximum chunk required far more RAM: 10G for tiny
        # 16 BAM panels and scaling at about 4x the rate of 31MB/BAM. We use an intercept
        # of 1000 MB to increase success probability for smaller panels with non-30x
        # average depth, which is probably a common scenario among users. These numbers are
        # after removing the very small window in chr2 that requires high memory.
        #
        # For convenience, we try the 99% criteria by default. If that fails, an
        # attempt will be made using the maximum parameters. Finally a 200G attempt will be
        # made. If none of these work, the chunk may have some very unusual characteristics
        # and need to be manually removed by the user.
        #
        # x = # BAMs (assumed to be mean 30x coverage)
        # attempt 1: 1000 + 31x      MB
        #         2: 10000 + 31*4*x  MB
        #         3: 200000          MB  (=200GB)
        # N.B. attempt starts at 1
        mem_mb=lambda wildcards, input, threads, attempt: 200000 if attempt == 3 else (1000 + (9000*(attempt-1)) + 31*(4**(attempt-1))*len(input))
    threads: 1
    shell:
        """
        sentieon driver -t {threads} \
            -r {config[ref]} \
            {params.regionflag} \
            {params.bamlist} \
            --algo Haplotyper \
            -d {config[dbsnp]} \
            --min_map_qual {params.mmq} \
            --min_base_qual {config[min_base_quality_score]} \
            --trim_soft_clip \
            {output.vcf} >& {log}
        """
